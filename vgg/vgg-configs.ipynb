{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG Configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: https://www.researchgate.net/publication/326989744_Unconventional_Wisdom_A_New_Transfer_Learning_Approach_Applied_to_Bengali_Numeral_Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with ALL layers frozen, except for the SOFTMAX layer\n",
    "def get_model_exp1_a():\n",
    "    base_model = VGG16(weights='imagenet',include_top=False,pooling='avg',input_shape=(48, 48, 3))\n",
    "    base_model.trainable = False\n",
    "    X = base_model.output\n",
    "    X.trainable = False\n",
    "    predictions = Dense(10, activation='softmax')(X)\n",
    "    # magical line of freezing layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable=False\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    model.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "model=get_model_exp1_a()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with all layers UNFROZEN, GPU might explode!\n",
    "def get_model_exp1_b():\n",
    "    base_model = VGG16(weights='imagenet',include_top=False,pooling='avg',input_shape=(48, 48, 3))\n",
    "    base_model.trainable = False\n",
    "    X = base_model.output\n",
    "    X.trainable = False\n",
    "    predictions = Dense(10, activation='softmax')(X)\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    model.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "model=get_model_exp1_b()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with only the SOFTMAX layer frozen, unconventional wisdom kicks in!\n",
    "def get_model_exp2_a():\n",
    "    base_model = VGG16(weights='imagenet',include_top=False,pooling='avg',input_shape=(48, 48, 3))\n",
    "    base_model.trainable = False\n",
    "    X = base_model.output\n",
    "    X.trainable = False\n",
    "    predictions = Dense(10, activation='softmax',trainable=False)(X)\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    model.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "model=get_model_exp2_a()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with SOFTMAX layer and last 4 vgg layers frozen, others layers remain unfrozen unconventional wisdom for the win!\n",
    "def get_model_exp2_b():\n",
    "    base_model = VGG16(weights='imagenet',include_top=False,pooling='avg',input_shape=(48, 48, 3))\n",
    "    base_model.trainable = False\n",
    "    X = base_model.output\n",
    "    X.trainable = False\n",
    "    predictions = Dense(10, activation='softmax',trainable=False)(X)\n",
    "    for layer in base_model.layers[-4:]:\n",
    "        layer.trainable=False\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    model.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "model=get_model_exp2_b()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_exp2_c():\n",
    "    base_model = VGG16(weights='imagenet',include_top=False,pooling='avg',input_shape=(48, 48, 3))\n",
    "    base_model.trainable = False\n",
    "    X = base_model.output\n",
    "    X.trainable = False\n",
    "    # last layer unfrozen here\n",
    "    predictions = Dense(10, activation='softmax',trainable=True)(X)\n",
    "    # intermediate layers fronzen\n",
    "    for layer in base_model.layers[-5:]:\n",
    "        layer.trainable=False\n",
    "    #vgg divided in two parts, trained jointly\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    model.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "model=get_model_exp2_c()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with only the SOFTMAX layer frozen, unconventional wisdom kicks in!\n",
    "def get_model_exp2_d():\n",
    "    base_model = VGG16(weights='imagenet',include_top=False,pooling='avg',input_shape=(48, 48, 3))\n",
    "    base_model.trainable = False\n",
    "    X = base_model.output\n",
    "    X.trainable = False\n",
    "    predictions = Dense(10, activation='softmax')(X)\n",
    "    #for layer in base_model.layers[::2]:\n",
    "    #        layer.trainable = False\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    weights = model.layers[-1].get_weights()\n",
    "   \n",
    "    w1 = np.zeros( weights[0].shape )\n",
    "    w2 = np.zeros( weights[1].shape )\n",
    "    w  = [w1,w2]\n",
    "    for i in range(0,10):\n",
    "        w1[i*50:(i+1)*50,i] = 1.0\n",
    "    print(w1.shape)\n",
    "    model.layers[-1].set_weights( w)\n",
    "    print(w2)\n",
    "    model.layers[-1].trainable=False\n",
    "    \n",
    "    \n",
    "    model.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "model=get_model_exp2_d()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "model=get_model_exp2_c()\n",
    "path_model='mnist2c-dg.h5' \n",
    "\n",
    "\n",
    "#model.load_weights(path_model)\n",
    "K.set_value(model.optimizer.lr,1e-2) # set the learning rate\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    rotation_range=35,\n",
    "    width_shift_range=0.25,\n",
    "    height_shift_range=0.25,\n",
    "    shear_range = 20,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied)\n",
    "datagen.fit(rgb_arr_to_3d)\n",
    "\n",
    "model.fit_generator(datagen.flow(rgb_arr_to_3d, Y_train, batch_size=128),\n",
    "                    steps_per_epoch=len(rgb_arr_to_3d) / 128, \n",
    "                    epochs=100, \n",
    "                    verbose=1, \n",
    "                    validation_data=(rgb_arr_to_3d_test,Y_test),\n",
    "                    callbacks=[\n",
    "                       ModelCheckpoint(filepath=path_model, monitor='val_acc',  save_best_only=False),\n",
    "                    ]      \n",
    "                   )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
